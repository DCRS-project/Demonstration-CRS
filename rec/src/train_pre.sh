accelerate launch train_pre.py --dataset redial --tokenizer ../utils/tokenizer/dialogpt-small --model microsoft/DialoGPT-small --text_tokenizer ../utils/tokenizer/roberta-base --text_encoder roberta-base --num_train_epochs 5 --gradient_accumulation_steps 1 --per_device_train_batch_size 64 --per_device_eval_batch_size 128 --num_warmup_steps 1389 --max_length 200 --prompt_max_length 200 --entity_max_length 32 --learning_rate 5e-4 --output_dir /mnt/wangxiaolei/crs/prompt/dialogpt_redial-resp_5e-4 --use_wandb --project crs-prompt-pre-final --name dialogpt_redial-resp_5e-4 --log_all
accelerate launch train_pre.py --dataset redial --tokenizer ../utils/tokenizer/dialogpt-small --model microsoft/DialoGPT-small --text_tokenizer ../utils/tokenizer/roberta-base --text_encoder roberta-base --num_train_epochs 5 --gradient_accumulation_steps 1 --per_device_train_batch_size 64 --per_device_eval_batch_size 128 --num_warmup_steps 1389 --max_length 200 --prompt_max_length 200 --entity_max_length 32 --learning_rate 1e-4 --output_dir /mnt/wangxiaolei/crs/prompt/dialogpt_redial-resp_1e-4 --use_wandb --project crs-prompt-pre-final --name dialogpt_redial-resp_1e-4 --log_all
accelerate launch train_pre.py --dataset redial --tokenizer ../utils/tokenizer/dialogpt-small --model microsoft/DialoGPT-small --text_tokenizer ../utils/tokenizer/roberta-base --text_encoder roberta-base --num_train_epochs 5 --gradient_accumulation_steps 1 --per_device_train_batch_size 64 --per_device_eval_batch_size 128 --num_warmup_steps 1389 --max_length 200 --prompt_max_length 200 --entity_max_length 32 --learning_rate 1e-3 --output_dir /mnt/wangxiaolei/crs/prompt/dialogpt_redial-resp_1e-3 --use_wandb --project crs-prompt-pre-final --name dialogpt_redial-resp_1e-3 --log_all
